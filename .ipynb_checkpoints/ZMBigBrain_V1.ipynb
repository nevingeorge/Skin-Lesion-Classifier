{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818b7d9d-9f45-4f52-8c4c-50463ad040cd",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34273557-991c-434d-b99e-828732b37147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x107721d50>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import datetime, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DataLoader = torch.utils.data.DataLoader\n",
    "\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd509cad-d79d-4ea0-9a85-6f1992a424c2",
   "metadata": {},
   "source": [
    "<h3>Set-up classes and mappings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a7f9c36-73c6-42ae-8616-e61216145720",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"ACK\", \"BCC\", \"MEL\", \"NEV\", \"SCC\", \"SEK\"]\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "idx_to_class = {idx: cls_name for cls_name, idx in class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc854c9-ef65-41a3-bccb-a10ade3ed39a",
   "metadata": {},
   "source": [
    "<h3>Pre-processing: cropping and lowering resolution, per paper.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b665afd9-cc8e-4785-8f42-567f710dee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img, crop_ratio):\n",
    "    width, height = img.size\n",
    "    new_size = int(crop_ratio * min(width, height))\n",
    "    left = (width - new_size) // 2\n",
    "    top = (height - new_size) // 2\n",
    "    right = left + new_size\n",
    "    bottom = top + new_size\n",
    "    return img.crop((left, top, right, bottom))\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: crop_center(img, 0.8)),\n",
    "    transforms.Resize((224, 224)), # 224x224 is a common choice for RESNET-18, I'm told.. \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126698e2-24c0-4f1f-a4cb-0118dd8952e0",
   "metadata": {},
   "source": [
    "<h3>Dataset Class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "accc927c-7b9b-4d44-b46e-7f0348194414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAD_UFES_Dataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dict, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dict = label_dict\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(img_dir) \n",
    "                            if f.endswith('.png') and f in label_dict]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        label_name = self.label_dict[img_name]\n",
    "        label = class_to_idx[label_name]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc7895-d359-4e4a-b428-6daf9214e9d2",
   "metadata": {},
   "source": [
    "<h3>Establish Paths</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ec5d2b94-7760-4765-9848-41cf9e90b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_path = '/Users/vedansh/Desktop/230PRJ/skin-lesion-classifier/runs'\n",
    "\n",
    "data_path = os.path.expanduser('~/Desktop/230PRJ/PAD-UFES-20/')\n",
    "metadata_path = os.path.join(data_path, 'metadata.csv')\n",
    "images_path = os.path.join(data_path, 'images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5065e37b-19e1-425f-8da5-d8a46c303454",
   "metadata": {},
   "source": [
    "<h3>Load Labels, Initialize and Split dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a660256b-29f5-41be-9af3-4400730f5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "label_dict = dict(zip(metadata['img_id'], metadata['diagnostic']))\n",
    "label_dict = {f\"{key}\": value for key, value in label_dict.items()}\n",
    "\n",
    "dataset = PAD_UFES_Dataset(img_dir=images_path, label_dict=label_dict, transform=transform)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Sanity check\n",
    "assert train_size + test_size + val_size == dataset_size == 2298"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e631706-4e97-48fb-b29a-2f1ead68cac0",
   "metadata": {},
   "source": [
    "<h3>Get class weights for later: potentially weighted softmax, or re-sampling, etc.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "222acc00-94a3-4a9b-9681-ad785729fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [label for _, label in train_dataset]\n",
    "label_counts = Counter(train_labels)\n",
    "total_samples = sum(label_counts.values())\n",
    "class_weights = [total_samples / label_counts[i] for i in range(len(classes))]\n",
    "class_weights = torch.FloatTensor(class_weights).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3744dc9-c717-4586-89f8-af3384c24de9",
   "metadata": {},
   "source": [
    "<h3>Set-up Data Loaders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "01ae875e-ebbc-4c68-ba81-a991941c3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c7b9d-4549-4e96-aa62-7394a545890f",
   "metadata": {},
   "source": [
    "<h3>Attempt to use RESNET-18</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1698f0de-cf35-402f-b528-3e3dae683b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(classes) == 6\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "# Unfreeze just the last CONV layer and last FC layer, and tweak it for 6 outputs\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c89c22-0762-4e3d-b169-f62d03630388",
   "metadata": {},
   "source": [
    "<h3>Loss function, optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc3d61e6-ff2e-4711-9e54-2262be7e6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights) # Weighted CEL\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # Keeping this the same as the paper.\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) # LR decay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b6d1f-0894-4a1b-b042-87b156c722d8",
   "metadata": {},
   "source": [
    "<h3>Training time</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6de3ef7f-f651-42a3-8af4-8bffda558c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 1.6622 Acc: 0.4314\n",
      "Val Loss: 1.5077 Acc: 0.4061\n",
      "Epoch 2/5\n",
      "Train Loss: 0.9383 Acc: 0.6464\n",
      "Val Loss: 1.1442 Acc: 0.4803\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5977 Acc: 0.7519\n",
      "Val Loss: 1.6003 Acc: 0.6681\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3357 Acc: 0.8618\n",
      "Val Loss: 1.7841 Acc: 0.6507\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2370 Acc: 0.9108\n",
      "Val Loss: 1.7126 Acc: 0.6070\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_acc = running_corrects.double() / train_size\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs = val_inputs.to(DEVICE)\n",
    "            val_labels = val_labels.to(DEVICE)\n",
    "            \n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "            \n",
    "            _, val_preds = torch.max(val_outputs, 1)\n",
    "            val_running_loss += val_loss.item() * val_inputs.size(0)\n",
    "            val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
    "    \n",
    "    val_loss = val_running_loss / val_size\n",
    "    val_acc = val_running_corrects.double() / val_size\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2cdbb-e306-4dd2-b2d4-00e89a396253",
   "metadata": {},
   "source": [
    "<h3>Testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bb9cec3d-3608-4c5f-b90a-65dbc05a29e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5758\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_running_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_inputs = test_inputs.to(DEVICE)\n",
    "        test_labels = test_labels.to(DEVICE)\n",
    "        \n",
    "        test_outputs = model(test_inputs)\n",
    "        _, test_preds = torch.max(test_outputs, 1)\n",
    "        test_running_corrects += torch.sum(test_preds == test_labels.data)\n",
    "\n",
    "test_acc = test_running_corrects.double() / test_size\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42916570-864e-43ba-98fb-0f759d43fdaa",
   "metadata": {},
   "source": [
    "<h3>Save RESNET-18</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c16016-5fab-4184-bd4c-0393cb8f7574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveity save save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66288745-2982-4441-b9f6-7e136e0597b4",
   "metadata": {},
   "source": [
    "<h3>Custom (Simple) CNN Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4cf9de71-f521-4e93-a67f-b397861ed278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # 3 CONV\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # 1 MAXPOOL\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 2 FC\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, len(classes))\n",
    "        # 1 DO\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # OP Dims: [batch_size, 32, 112, 112]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # OP Dims: [batch_size, 64, 56, 56]\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # OP Dims: [batch_size, 128, 28, 28]\n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9bb959-4db4-4414-abc0-d71f02c40f75",
   "metadata": {},
   "source": [
    "<h3>Set this up now.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabefad3-9c7a-4395-9a96-dc1194d7ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN(num_classes=len(classes))\n",
    "assert len(classes) == 6, \"you're fucked.\"\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2080e-e776-4b3c-a35e-4d92c91d8a12",
   "metadata": {},
   "source": [
    "<h3>Train SIMPLE CNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4c52ce7c-7a55-431c-bc30-19eb3d133de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.1054 Acc: 0.9532\n",
      "Val Loss: 1.9611 Acc: 0.6725\n",
      "Epoch 2/5\n",
      "Train Loss: 0.0321 Acc: 0.9880\n",
      "Val Loss: 1.7768 Acc: 0.6681\n",
      "Epoch 3/5\n",
      "Train Loss: 0.0454 Acc: 0.9793\n",
      "Val Loss: 1.9663 Acc: 0.6070\n",
      "Epoch 4/5\n",
      "Train Loss: 0.0524 Acc: 0.9761\n",
      "Val Loss: 2.0009 Acc: 0.6900\n",
      "Epoch 5/5\n",
      "Train Loss: 0.0621 Acc: 0.9739\n",
      "Val Loss: 1.9509 Acc: 0.6856\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Spring forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Fall back\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_acc = running_corrects.double() / train_size\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs = val_inputs.to(DEVICE)\n",
    "            val_labels = val_labels.to(DEVICE)\n",
    "            \n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "            \n",
    "            _, val_preds = torch.max(val_outputs, 1)\n",
    "            val_running_loss += val_loss.item() * val_inputs.size(0)\n",
    "            val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
    "    \n",
    "    val_loss = val_running_loss / val_size\n",
    "    val_acc = val_running_corrects.double() / val_size\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e2f42-90d7-43b4-99cf-ff38b771c407",
   "metadata": {},
   "source": [
    "<h3>Test SIMPLE CNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e7a23118-4cbf-430a-abc7-bc35abb7837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6450\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_running_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_inputs = test_inputs.to(DEVICE)\n",
    "        test_labels = test_labels.to(DEVICE)\n",
    "        \n",
    "        test_outputs = model(test_inputs)\n",
    "        _, test_preds = torch.max(test_outputs, 1)\n",
    "        test_running_corrects += torch.sum(test_preds == test_labels.data)\n",
    "\n",
    "test_acc = test_running_corrects.double() / test_size\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85c8ac-b8d3-43da-a986-f89a7e0b40ba",
   "metadata": {},
   "source": [
    "<h3>Save SIMPLE CNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d98c77-1a52-4c5a-b3bb-76af6b747294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save l8r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
