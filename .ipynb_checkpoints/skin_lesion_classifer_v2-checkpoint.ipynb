{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818b7d9d-9f45-4f52-8c4c-50463ad040cd",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34273557-991c-434d-b99e-828732b37147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import datetime, os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import ConcatDataset, Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.models as models\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score, MulticlassConfusionMatrix\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "DataLoader = torch.utils.data.DataLoader\n",
    "\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd509cad-d79d-4ea0-9a85-6f1992a424c2",
   "metadata": {},
   "source": [
    "<h3>Set-up classes and mappings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a7f9c36-73c6-42ae-8616-e61216145720",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"ACK\", \"BCC\", \"MEL\", \"NEV\", \"SCC\", \"SEK\"]\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "idx_to_class = {idx: cls_name for cls_name, idx in class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc854c9-ef65-41a3-bccb-a10ade3ed39a",
   "metadata": {},
   "source": [
    "<h3>Pre-processing: cropping and lowering resolution, per paper.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b665afd9-cc8e-4785-8f42-567f710dee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img, crop_ratio):\n",
    "    width, height = img.size\n",
    "    new_size = int(crop_ratio * min(width, height))\n",
    "    left = (width - new_size) // 2\n",
    "    top = (height - new_size) // 2\n",
    "    right = left + new_size\n",
    "    bottom = top + new_size\n",
    "    return img.crop((left, top, right, bottom))\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: crop_center(img, 0.8)),\n",
    "    transforms.Resize((224, 224)), # 224x224 is a common choice for RESNET-18, I'm told..  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_flip = transforms.Compose([\n",
    "    transform,\n",
    "    transforms.RandomHorizontalFlip(.5), # .5 is used in the Pacheco code \n",
    "    transforms.RandomVerticalFlip(.2), # .2 is used in the Pacheco code \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126698e2-24c0-4f1f-a4cb-0118dd8952e0",
   "metadata": {},
   "source": [
    "<h3>Dataset Class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accc927c-7b9b-4d44-b46e-7f0348194414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAD_UFES_Dataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dict, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dict = label_dict\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(img_dir) \n",
    "                            if f.endswith('.png') and f in label_dict]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        label_name = self.label_dict[img_name]\n",
    "        label = class_to_idx[label_name]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc7895-d359-4e4a-b428-6daf9214e9d2",
   "metadata": {},
   "source": [
    "<h3>Establish Paths</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec5d2b94-7760-4765-9848-41cf9e90b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = os.path.expanduser('~/Desktop/230PRJ/PAD-UFES-20/')\n",
    "data_path = os.path.expanduser('~/Desktop/CS230/Project/PAD-UFES-20/')\n",
    "metadata_path = os.path.join(data_path, 'metadata.csv')\n",
    "images_path = os.path.join(data_path, 'images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5065e37b-19e1-425f-8da5-d8a46c303454",
   "metadata": {},
   "source": [
    "<h3>Load Labels, Initialize and Split dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a660256b-29f5-41be-9af3-4400730f5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "label_dict = dict(zip(metadata['img_id'], metadata['diagnostic']))\n",
    "label_dict = {f\"{key}\": value for key, value in label_dict.items()}\n",
    "\n",
    "dataset = PAD_UFES_Dataset(img_dir=images_path, label_dict=label_dict, transform=transform)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_dataset.transform = transform_flip # apply flip transforms just to train set\n",
    "\n",
    "# Sanity check\n",
    "assert train_size + test_size + val_size == dataset_size == 2298"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e631706-4e97-48fb-b29a-2f1ead68cac0",
   "metadata": {},
   "source": [
    "<h3>Get class weights for later: potentially weighted softmax, or re-sampling, etc.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "222acc00-94a3-4a9b-9681-ad785729fd8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m [label \u001b[38;5;28;01mfor\u001b[39;00m _, label \u001b[38;5;129;01min\u001b[39;00m train_dataset]\n\u001b[1;32m      2\u001b[0m label_counts \u001b[38;5;241m=\u001b[39m Counter(train_labels)\n\u001b[1;32m      3\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(label_counts\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/opt/miniconda3/envs/230project/lib/python3.12/site-packages/torch/utils/data/dataset.py:412\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]]\n",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m, in \u001b[0;36mPAD_UFES_Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m label \u001b[38;5;241m=\u001b[39m class_to_idx[label_name]\n\u001b[1;32m     16\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, img_name)\n\u001b[0;32m---> 17\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     19\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/230project/lib/python3.12/site-packages/PIL/Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/230project/lib/python3.12/site-packages/PIL/ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_labels = [label for _, label in train_dataset]\n",
    "label_counts = Counter(train_labels)\n",
    "total_samples = sum(label_counts.values())\n",
    "class_weights = [total_samples / label_counts[i] for i in range(len(classes))]\n",
    "class_weights = torch.FloatTensor(class_weights).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3744dc9-c717-4586-89f8-af3384c24de9",
   "metadata": {},
   "source": [
    "<h3>Set-up Data Loaders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae875e-ebbc-4c68-ba81-a991941c3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee958ac-a88e-4160-8b87-e88c796d6d2d",
   "metadata": {},
   "source": [
    "<h3>Display some images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80d6e3-897a-423e-9c1c-dd13c38dfcbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "images, _ = next(iter(train_loader))\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca49ce-6885-4151-b799-42fa24d8bf8b",
   "metadata": {},
   "source": [
    "<h1>Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c7b9d-4549-4e96-aa62-7394a545890f",
   "metadata": {},
   "source": [
    "<h3>ResNet-18</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698f0de-cf35-402f-b528-3e3dae683b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(classes) == 6\n",
    "\n",
    "model_resnet18 = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_resnet18.fc.in_features\n",
    "model_resnet18.fc = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "# Unfreeze just the last CONV layer and last FC layer, and tweak it for 6 outputs\n",
    "for param in model_resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_resnet18.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_resnet18.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_resnet18 = model_resnet18.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d7010-68fb-4bd9-9f47-a7790cd0a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_resnet18 = nn.CrossEntropyLoss(weight=class_weights) # Weighted CEL\n",
    "optimizer_resnet18 = optim.SGD(model_resnet18.parameters(), lr=0.01, momentum=0.9) # Keeping this the same as the paper.\n",
    "scheduler_resnet18 = optim.lr_scheduler.StepLR(optimizer_resnet18, step_size=7, gamma=0.1) # LR decay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba30a7-e5b5-4b80-adc6-6afe827fa371",
   "metadata": {},
   "source": [
    "<h3>ResNet-50</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f558daf-78d3-4b3f-9d18-95d5ed40c5cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(classes) == 6\n",
    "\n",
    "model_resnet50 = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_resnet50.fc.in_features\n",
    "model_resnet50.fc = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "# Unfreeze just the last CONV layer and last FC layer, and tweak it for 6 outputs\n",
    "for param in model_resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_resnet50.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_resnet50.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_resnet50 = model_resnet50.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a82954-7e6a-4078-a128-1c52ae868ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_resnet50 = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer_resnet50 = optim.SGD(model_resnet50.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001)\n",
    "scheduler_resnet50 = optim.lr_scheduler.StepLR(optimizer_resnet50, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d30d3-00b9-4954-94a9-004b708a8c25",
   "metadata": {},
   "source": [
    "<h3>EfficientNet B4</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac0cc5-86d6-4a9c-879a-2a254055f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(classes) == 6\n",
    "\n",
    "model_efficientnet_b4 = models.efficientnet_b4(pretrained=True)\n",
    "\n",
    "for param in model_efficientnet_b4.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_efficientnet_b4.features[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_efficientnet_b4.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_efficientnet_b4 = model_efficientnet_b4.to(DEVICE)\n",
    "\n",
    "model_efficientnet_b4.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.4, inplace=True),\n",
    "    nn.Linear(model_efficientnet_b4.classifier[1].in_features, len(classes))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684a638-fb88-44fb-bfc0-4b61e799ba40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion_efficientnet_b4 = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer_efficientnet_b4 = optim.SGD(model_efficientnet_b4.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler_efficientnet_b4 = optim.lr_scheduler.StepLR(optimizer_efficientnet_b4, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66288745-2982-4441-b9f6-7e136e0597b4",
   "metadata": {},
   "source": [
    "<h3>Custom (Simple) CNN Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9de71-f521-4e93-a67f-b397861ed278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # 3 CONV\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # 1 MAXPOOL\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 2 FC\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, len(classes))\n",
    "        # 1 DO\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # OP Dims: [batch_size, 32, 112, 112]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # OP Dims: [batch_size, 64, 56, 56]\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # OP Dims: [batch_size, 128, 28, 28]\n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabefad3-9c7a-4395-9a96-dc1194d7ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleCNN = SimpleCNN(num_classes=len(classes))\n",
    "assert len(classes) == 6, \"incorrect number of classes\"\n",
    "simpleCNN = simpleCNN.to(DEVICE)\n",
    "\n",
    "criterion_simpleCNN = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer_simpleCNN = optim.Adam(simpleCNN.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9f33d-9728-4dca-9864-4df8392f54d3",
   "metadata": {},
   "source": [
    "<h3>ResNet-18 with Dropout</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ea205-76e9-42ad-be5f-e384da32d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(classes) == 6\n",
    "\n",
    "model_resnet18WithDropout = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_resnet18WithDropout.fc.in_features\n",
    "model_resnet18WithDropout.fc = nn.Sequential(nn.Flatten(),\n",
    "                                             nn.Dropout(0.5),\n",
    "                                             nn.Linear(num_ftrs, 256),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Dropout(0.5),\n",
    "                                             nn.Linear(256, len(classes)))\n",
    "\n",
    "for param in model_resnet18WithDropout.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_resnet18WithDropout.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_resnet18WithDropout.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_resnet18WithDropout = model_resnet18WithDropout.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7994ae-6cdd-4be1-972a-f1645aa222de",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_resnet18WithDropout = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer_resnet18WithDropout = optim.Adam(model_resnet18WithDropout.parameters(), lr=0.01, weight_decay=0.001) # use Adam and weight decay\n",
    "scheduler_resnet18WithDropout = optim.lr_scheduler.StepLR(optimizer_resnet18WithDropout, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be47e9c-5587-41b9-be52-139a25edd97f",
   "metadata": {},
   "source": [
    "<h3>ResNet-50 with Dropout</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52c398-8640-4d84-8ddf-9550cc433f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(classes) == 6\n",
    "\n",
    "model_resnet50WithDropout = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_resnet50WithDropout.fc.in_features\n",
    "model_resnet50WithDropout.fc = nn.Sequential(nn.Flatten(),\n",
    "                                             nn.Dropout(0.5),\n",
    "                                             nn.Linear(num_ftrs, 256),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Dropout(0.5),\n",
    "                                             nn.Linear(256, len(classes)))\n",
    "\n",
    "for param in model_resnet50WithDropout.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_resnet50WithDropout.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_resnet50WithDropout.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_resnet50WithDropout = model_resnet50WithDropout.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a42470-a26a-46d5-942d-c339f61883d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_resnet50WithDropout = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer_resnet50WithDropout = optim.Adam(model_resnet50WithDropout.parameters(), lr=0.01, weight_decay=0.001) # use Adam and weight decay\n",
    "scheduler_resnet50WithDropout = optim.lr_scheduler.StepLR(optimizer_resnet50WithDropout, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5deb6-a8e5-4b97-8ad5-b4d81589e4c5",
   "metadata": {},
   "source": [
    "<h3>Helper Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51e28a-c892-4689-bca3-8eaf50c86558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_name, model, train_loader, optimizer, criterion, scheduler, num_epochs):\n",
    "    writer = SummaryWriter('runs/' + log_name + '_' + str(datetime.datetime.now()).replace(\" \", \"_\"))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs = val_inputs.to(DEVICE)\n",
    "                val_labels = val_labels.to(DEVICE)\n",
    "                \n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                \n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_running_loss += val_loss.item() * val_inputs.size(0)\n",
    "                val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
    "        \n",
    "        val_loss = val_running_loss / val_size\n",
    "        val_acc = val_running_corrects.double() / val_size\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch+1)\n",
    "        writer.add_scalar('Loss/validation', val_loss, epoch+1)\n",
    "        writer.add_scalar('Accuracy/train', epoch_acc, epoch+1)\n",
    "        writer.add_scalar('Accuracy/validation', val_acc, epoch+1)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067ae52-ee54-424e-9bb2-939f37c7aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(log_name, model, test_loader):\n",
    "    writer = SummaryWriter('runs/' + 'test/' + log_name + '_' + str(datetime.datetime.now()).replace(\" \", \"_\"))\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_inputs = test_inputs.to(DEVICE)\n",
    "            test_labels = test_labels.to(DEVICE)\n",
    "            \n",
    "            test_outputs = model(test_inputs)\n",
    "            _, test_preds = torch.max(test_outputs, 1)\n",
    "            all_preds.append(test_preds)\n",
    "            all_labels.append(test_labels)\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    num_classes = len(classes)\n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=num_classes, average='macro')\n",
    "    precision_metric = MulticlassPrecision(num_classes=num_classes, average='macro')\n",
    "    recall_metric = MulticlassRecall(num_classes=num_classes, average='macro')\n",
    "    f1_metric = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
    "    confusion_matrix_metric = MulticlassConfusionMatrix(num_classes=num_classes)\n",
    "\n",
    "    accuracy = accuracy_metric(all_preds, all_labels)\n",
    "    precision = precision_metric(all_preds, all_labels)\n",
    "    recall = recall_metric(all_preds, all_labels)\n",
    "    f1_score = f1_metric(all_preds, all_labels)\n",
    "    confusion_matrix = confusion_matrix_metric(all_preds, all_labels)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "    writer.add_scalar('Accuracy/test', accuracy)\n",
    "    writer.add_scalar('Precision/test', precision)\n",
    "    writer.add_scalar('Recall/test', recall)\n",
    "    writer.add_scalar('F1_score/test', f1_score)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    writer.add_figure('Confusion Matrix', plt.gcf(), global_step=0)\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b6d1f-0894-4a1b-b042-87b156c722d8",
   "metadata": {},
   "source": [
    "<h3>Training Time</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f62290-e0e8-4199-8eaf-1a765a216ad4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(\"ResNet18\", model_resnet18, train_loader, optimizer_resnet18, criterion_resnet18, scheduler_resnet18, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b95fc0-1f47-46e4-9116-a0b21490f035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(\"ResNet50\", model_resnet50, train_loader, optimizer_resnet50, criterion_resnet50, scheduler_resnet50, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17115c62-d90c-4cc0-a306-05317c2900cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(\"EfficientNetB4\", model_efficientnet_b4, train_loader, optimizer_efficientnet_b4, criterion_efficientnet_b4, scheduler_efficientnet_b4, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f73e28-5951-4da0-ac0c-515154ed6f27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(\"simpleCNN\", simpleCNN, train_loader, optimizer_simpleCNN, criterion_simpleCNN, None, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77557736-b199-4567-b4f5-43cf2077cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"ResNet18WithDropout\", model_resnet18WithDropout, train_loader, optimizer_resnet18WithDropout, criterion_resnet18WithDropout, scheduler_resnet18WithDropout, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518bb3a0-ed6d-4072-b9b4-3aae43b2a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"ResNet50WithDropout\", model_resnet50WithDropout, train_loader, optimizer_resnet50WithDropout, criterion_resnet50WithDropout, scheduler_resnet50WithDropout, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2cdbb-e306-4dd2-b2d4-00e89a396253",
   "metadata": {},
   "source": [
    "<h3>Testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5d3cb-2c4d-4899-994a-415cfa82c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(\"ResNet18\", model_resnet18, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8b5be-39fe-48c8-ad36-fb78f4dea49d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(\"ResNet50\", model_resnet50, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699d9cb-5fbf-4f2d-ba90-bf6133222153",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(\"EfficientNetB4\", model_efficientnet_b4, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f2393-a06f-4aae-835e-cf9852623fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(\"simpleCNN\", simpleCNN, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a436c-4fd3-4239-a184-0672b7b9667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(\"ResNet18WithDropout\", model_resnet18WithDropout, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceabaf1-e170-4da0-bf4b-39cebaaa178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(\"ResNet50WithDropout\", model_resnet50WithDropout, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94e62f-ebd8-40f4-b7ff-7eb863c7c185",
   "metadata": {},
   "source": [
    "<h3>Results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf25a8-ba3f-47cb-97f9-63bf9609674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a6a02-0f55-4457-b8d6-62735978a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
